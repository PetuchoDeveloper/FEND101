"""
Benchmark: ComparaciÃ³n de diferentes configuraciones de Connection Pool

Este script demuestra el impacto de diferentes lÃ­mites de pool:
- Pool pequeÃ±o (5 conexiones): Bottleneck esperado
- Pool medio (20 conexiones): Balance adecuado
- Pool grande (ilimitado): Sin lÃ­mites

Escenario: 50 peticiones concurrentes con server delay de 100ms
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any
import aiohttp
from smart_session import SmartSession


# ConfiguraciÃ³n del servidor mock
MOCK_SERVER_URL = "http://127.0.0.1:8888"
NUM_REQUESTS = 50
SERVER_DELAY_MS = 100


async def configure_mock_server(delay_ms: int):
    """Configura el delay del servidor mock"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{MOCK_SERVER_URL}/config",
                json={"latency_ms": delay_ms},
                timeout=aiohttp.ClientTimeout(total=5)
            ) as response:
                if response.status == 200:
                    config = await response.json()
                    print(f"âœ… Servidor mock configurado: {config['latency_ms']}ms delay")
                    return True
    except Exception as e:
        print(f"âŒ Error configurando servidor mock: {e}")
        print(f"   AsegÃºrate de que el servidor estÃ© corriendo en {MOCK_SERVER_URL}")
        return False


async def check_mock_server():
    """Verifica que el servidor mock estÃ© disponible"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{MOCK_SERVER_URL}/config",
                timeout=aiohttp.ClientTimeout(total=2)
            ) as response:
                if response.status == 200:
                    return True
    except:
        return False


async def make_request(session: SmartSession, request_id: int) -> Dict[str, Any]:
    """
    Hace una peticiÃ³n individual y registra mÃ©tricas.
    
    Returns:
        dict: {
            "request_id": int,
            "duration": float (seconds),
            "success": bool,
            "error": str (if failed)
        }
    """
    start_time = time.time()
    
    try:
        async with session.get(
            f"{MOCK_SERVER_URL}/test",
            timeout=aiohttp.ClientTimeout(total=30)
        ) as response:
            await response.json()
            duration = time.time() - start_time
            
            return {
                "request_id": request_id,
                "duration": duration,
                "success": True
            }
    
    except Exception as e:
        duration = time.time() - start_time
        return {
            "request_id": request_id,
            "duration": duration,
            "success": False,
            "error": str(e)
        }


async def run_benchmark(
    pool_size: int,
    num_requests: int,
    pool_name: str
) -> Dict[str, Any]:
    """
    Ejecuta un benchmark con una configuraciÃ³n especÃ­fica de pool.
    
    Args:
        pool_size: LÃ­mite de conexiones del pool
        num_requests: NÃºmero de peticiones concurrentes
        pool_name: Nombre descriptivo del benchmark
    
    Returns:
        dict: Resultados del benchmark con mÃ©tricas
    """
    print(f"\n{'='*60}")
    print(f"Benchmark: {pool_name}")
    print(f"Pool size: {pool_size}, Requests: {num_requests}")
    print(f"{'='*60}\n")
    
    # Crear sesiÃ³n con configuraciÃ³n especÃ­fica
    async with SmartSession(
        max_connections=pool_size,
        max_connections_per_host=pool_size,
        enable_monitoring=True,
        health_check_interval=2.0
    ) as session:
        
        # Iniciar todas las peticiones concurrentemente
        start_time = time.time()
        
        tasks = [
            make_request(session, i)
            for i in range(num_requests)
        ]
        
        results = await asyncio.gather(*tasks)
        
        total_time = time.time() - start_time
        
        # Obtener estadÃ­sticas finales del pool
        pool_stats = session.get_pool_stats()
        
        # Procesar resultados
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        
        durations = [r["duration"] for r in successful]
        
        # Calcular mÃ©tricas
        metrics = {
            "pool_name": pool_name,
            "pool_size": pool_size,
            "num_requests": num_requests,
            "total_time": total_time,
            "throughput_rps": num_requests / total_time if total_time > 0 else 0,
            "successful": len(successful),
            "failed": len(failed),
            "latency": {
                "min": min(durations) * 1000 if durations else 0,
                "max": max(durations) * 1000 if durations else 0,
                "mean": statistics.mean(durations) * 1000 if durations else 0,
                "median": statistics.median(durations) * 1000 if durations else 0,
                "p95": statistics.quantiles(durations, n=20)[18] * 1000 if len(durations) >= 20 else (max(durations) * 1000 if durations else 0),
                "p99": statistics.quantiles(durations, n=100)[98] * 1000 if len(durations) >= 100 else (max(durations) * 1000 if durations else 0),
            },
            "pool_stats": pool_stats
        }
        
        # Imprimir reporte
        await session.print_pool_report()
        
        print(f"\nğŸ“Š Resultados:")
        print(f"   Tiempo total:      {total_time:.2f}s")
        print(f"   Throughput:        {metrics['throughput_rps']:.1f} req/s")
        print(f"   Exitosas:          {len(successful)}/{num_requests}")
        print(f"   Fallidas:          {len(failed)}")
        print(f"\nâ±ï¸  Latencia:")
        print(f"   Min:               {metrics['latency']['min']:.1f}ms")
        print(f"   Mean:              {metrics['latency']['mean']:.1f}ms")
        print(f"   Median:            {metrics['latency']['median']:.1f}ms")
        print(f"   P95:               {metrics['latency']['p95']:.1f}ms")
        print(f"   P99:               {metrics['latency']['p99']:.1f}ms")
        print(f"   Max:               {metrics['latency']['max']:.1f}ms")
        
        return metrics


def print_comparison_table(results: List[Dict[str, Any]]):
    """Imprime una tabla comparativa de todos los benchmarks"""
    
    print("\n")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                    COMPARACIÃ“N DE CONFIGURACIONES DE POOL                      â•‘")
    print("â•‘                      50 Requests Concurrentes | 100ms Server Delay            â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print("â•‘ Config        â”‚ Time(s) â”‚  RPS   â”‚ P95(ms) â”‚ P99(ms) â”‚ Created â”‚ Reused â”‚ Idle â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•£")
    
    for r in results:
        pool_name = r['pool_name'].ljust(13)
        time_val = f"{r['total_time']:.2f}".rjust(7)
        rps_val = f"{r['throughput_rps']:.1f}".rjust(6)
        p95_val = f"{r['latency']['p95']:.0f}".rjust(7)
        p99_val = f"{r['latency']['p99']:.0f}".rjust(7)
        created_val = f"{r['pool_stats']['metrics']['created']}".rjust(7)
        reused_val = f"{r['pool_stats']['metrics']['reused']}".rjust(6)
        idle_val = f"{r['pool_stats']['idle']}".rjust(4)
        
        print(f"â•‘ {pool_name} â”‚ {time_val} â”‚ {rps_val} â”‚ {p95_val} â”‚ {p99_val} â”‚ {created_val} â”‚ {reused_val} â”‚ {idle_val} â•‘")
    
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # AnÃ¡lisis comparativo
    print("\nğŸ“ˆ AnÃ¡lisis:")
    
    baseline = results[0]
    for i, r in enumerate(results[1:], 1):
        speedup = baseline['total_time'] / r['total_time']
        print(f"\n   {r['pool_name']} vs {baseline['pool_name']}:")
        print(f"   - Speedup: {speedup:.2f}x mÃ¡s rÃ¡pido")
        print(f"   - Conexiones reutilizadas: {r['pool_stats']['metrics']['reused']}")
        
        if r['pool_stats']['metrics']['created'] > r['pool_size']:
            print(f"   âš ï¸  Se crearon mÃ¡s conexiones ({r['pool_stats']['metrics']['created']}) que el lÃ­mite del pool ({r['pool_size']})")
            print(f"      Esto indica que hubo cierre y reapertura de conexiones")


async def main():
    """FunciÃ³n principal del benchmark"""
    
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     Benchmark de Connection Pool - ACT10 AI               â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # Verificar servidor mock
    print("\nğŸ” Verificando servidor mock...")
    if not await check_mock_server():
        print("\nâŒ ERROR: Servidor mock no disponible")
        print("\n   Ejecuta en otra terminal:")
        print("   cd 'Semana III/ACT9 AI'")
        print("   python benchmark_mock_server.py")
        return
    
    print("âœ… Servidor mock detectado")
    
    # Configurar delay del servidor
    if not await configure_mock_server(SERVER_DELAY_MS):
        return
    
    # Definir configuraciones a probar
    benchmarks = [
        {"pool_size": 5, "name": "Small (5)"},
        {"pool_size": 20, "name": "Medium (20)"},
        {"pool_size": 1000, "name": "Unlimited"},
    ]
    
    # Ejecutar benchmarks
    results = []
    
    for config in benchmarks:
        result = await run_benchmark(
            pool_size=config["pool_size"],
            num_requests=NUM_REQUESTS,
            pool_name=config["name"]
        )
        results.append(result)
        
        # Pausa entre benchmarks para limpiar estado
        await asyncio.sleep(2)
    
    # Imprimir tabla comparativa
    print_comparison_table(results)
    
    # Recomendaciones
    print("\nğŸ’¡ Recomendaciones:")
    print("\n   1. Pool pequeÃ±o (5 conexiones):")
    print("      âœ“ Usa menos recursos del sistema")
    print("      âœ— Baja throughput con alta concurrencia")
    print("      â†’ Ideal para: APIs con rate limiting estricto")
    
    print("\n   2. Pool medio (20 conexiones):")
    print("      âœ“ Balance entre rendimiento y recursos")
    print("      âœ“ Buen throughput para la mayorÃ­a de casos")
    print("      â†’ Ideal para: Aplicaciones tÃ­picas de producciÃ³n")
    
    print("\n   3. Pool ilimitado (1000 conexiones):")
    print("      âœ“ MÃ¡ximo throughput posible")
    print("      âœ— Alto overhead de sistema (sockets, memoria)")
    print("      âœ— Crea nueva conexiÃ³n por cada request (no reutiliza)")
    print("      â†’ Ideal para: RÃ¡fagas cortas de alta concurrencia")
    
    print("\n   ğŸ¯ RecomendaciÃ³n para EcoMarket:")
    best = results[1]  # Medium pool
    print(f"      Pool de {best['pool_size']} conexiones")
    print(f"      Throughput: {best['throughput_rps']:.1f} req/s")
    print(f"      Latencia P95: {best['latency']['p95']:.0f}ms")
    print(f"      Conexiones reutilizadas: {best['pool_stats']['metrics']['reused']}")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Benchmark interrumpido por el usuario")
