# An√°lisis Comparativo: Modelos de Asincron√≠a para Clientes HTTP

## Introducci√≥n

Este documento explica y compara tres modelos de concurrencia aplicados a clientes HTTP usando el caso de EcoMarket. El escenario es siempre el mismo: **cargar simult√°neamente productos, categor√≠as y perfil de usuario**.

---

## 1. Modelo de Callbacks (concurrent.futures)

### C√≥mo funciona

```python
with ThreadPoolExecutor(max_workers=3) as executor:
    # 1. Lanzar peticiones (retornan Future objects)
    future_productos = executor.submit(hacer_peticion_productos)
    future_categorias = executor.submit(hacer_peticion_categorias)
    future_perfil = executor.submit(hacer_peticion_perfil)
    
    # 2. Registrar callbacks que se ejecutan cuando cada una termina
    future_productos.add_done_callback(callback_exito)
    future_categorias.add_done_callback(callback_exito)
    future_perfil.add_done_callback(callback_exito)
    
    # 3. Esperar a que todas completen
    for future in as_completed([future_productos, future_categorias, future_perfil]):
        pass  # Los callbacks ya se ejecutaron
```

### Componentes clave

1. **`.submit()`**: Lanza la tarea en un thread del pool, retorna `Future` inmediatamente
2. **`.add_done_callback(funci√≥n)`**: Registra una funci√≥n que se ejecuta autom√°ticamente cuando el Future termina
3. **Callback**: Funci√≥n que recibe el `Future` como argumento y puede obtener el resultado con `.result()`

### Flujo de ejecuci√≥n

```mermaid
graph TD
    A[Programa Principal] -->|submit| B[Thread Pool]
    B -->|Future 1| C[GET /productos]
    B -->|Future 2| D[GET /categorias]
    B -->|Future 3| E[GET /perfil]
    C -->|termina| F[Callback 1]
    D -->|termina| G[Callback 2]
    E -->|termina| H[Callback 3]
    F --> I[Procesar resultado]
    G --> I
    H --> I
```

### ¬øQu√© pasa si /categorias falla con timeout?

```python
def callback_con_manejo_error(future):
    try:
        resultado = future.result()
        print(f"‚úÖ '{resultado['endpoint']}' completado")
    except requests.Timeout:
        print(f"‚è±Ô∏è TIMEOUT - Las dem√°s peticiones siguen normalmente")
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
```

**Respuesta**: Las dem√°s peticiones **NO se enteran** del error. Cada callback maneja su propio error independientemente.

### Ventajas espec√≠ficas para clientes HTTP

| Ventaja | Explicaci√≥n |
|---------|-------------|
| ‚úÖ **Control granular** | Puedes procesar cada respuesta HTTP apenas llegue (sin esperar a las dem√°s) |
| ‚úÖ **Errores independientes** | Una petici√≥n con timeout no afecta las dem√°s |
| ‚úÖ **Reactividad** | Ideal para mostrar datos en UI conforme van llegando |

### Desventajas espec√≠ficas para clientes HTTP

| Desventaja | Explicaci√≥n |
|-----------|-------------|
| ‚ùå **Callback Hell** | Si necesitas hacer peticiones secuenciales (una depende de otra), el c√≥digo se anida |
| ‚ùå **C√≥digo verboso** | Necesitas definir funciones callback separadas |
| ‚ùå **Flujo difuso** | El orden de ejecuci√≥n no es obvio al leer el c√≥digo |

---

## 2. Modelo de Futures (ThreadPoolExecutor)

### C√≥mo funciona

```python
with ThreadPoolExecutor(max_workers=3) as executor:
    # 1. Lanzar peticiones y guardar los Futures
    futures = {
        executor.submit(hacer_peticion_productos): "productos",
        executor.submit(hacer_peticion_categorias): "categorias",
        executor.submit(hacer_peticion_perfil): "perfil"
    }
    
    # 2. Procesar resultados conforme terminan
    for future in as_completed(futures):
        endpoint = futures[future]
        try:
            resultado = future.result()  # Obtiene el valor (bloquea si no termin√≥)
            print(f"‚úÖ {endpoint}: {len(resultado['data'])} items")
        except Exception as e:
            print(f"‚ùå {endpoint}: {e}")
```

### Componentes clave

1. **`Future` object**: "Promesa" de un resultado futuro
2. **`.result()`**: Obtiene el valor (bloquea hasta que termine si a√∫n no lo hizo)
3. **`as_completed(futures)`**: Iterador que retorna Futures en orden de terminaci√≥n
4. **`wait(futures)`**: Espera a un conjunto completo de Futures

### M√©todos de espera

| M√©todo | Comportamiento |
|--------|----------------|
| `as_completed()` | Retorna Futures conforme van terminando (streaming) |
| `wait(return_when=ALL_COMPLETED)` | Espera a que TODOS terminen |
| `wait(return_when=FIRST_COMPLETED)` | Retorna apenas UNO termine |

### ¬øC√≥mo manejar el error de UN future sin perder los dem√°s?

```python
resultados_validos = []
errores = []

for future in as_completed(futures):
    endpoint = futures[future]
    try:
        resultado = future.result()
        resultados_validos.append(resultado)
    except requests.Timeout:
        errores.append({"endpoint": endpoint, "error": "Timeout"})
        # Continuamos con los dem√°s
```

**Clave**: Cada `.result()` se envuelve en `try/except` individualmente. Los errores no se propagan.

### Ventajas espec√≠ficas para clientes HTTP

| Ventaja | Explicaci√≥n |
|---------|-------------|
| ‚úÖ **C√≥digo m√°s expl√≠cito** | Ves claramente qu√© Futures esperar |
| ‚úÖ **Consulta de estado** | Puedes verificar `.done()`, `.running()`, `.cancelled()` |
| ‚úÖ **Flexibilidad de espera** | Espera a "todos" o "el primero que termine" |
| ‚úÖ **Cancelaci√≥n** | Puedes cancelar peticiones que a√∫n no empezaron |

### Desventajas espec√≠ficas para clientes HTTP

| Desventaja | Explicaci√≥n |
|-----------|-------------|
| ‚ùå **`.result()` bloquea** | Si llamas antes de que termine, bloquea el thread actual |
| ‚ùå **M√°s boilerplate** | Necesitas iterar sobre Futures y manejar errores manualmente |
| ‚ùå **Menos reactivo que callbacks** | Debes "preguntar" activamente por resultados |

---

## 3. Modelo Async/Await (asyncio + aiohttp)

### C√≥mo funciona

```python
async def cargar_datos():
    async with aiohttp.ClientSession(timeout=timeout) as session:
        # 1. Lanzar las 3 coroutines EN PARALELO
        resultados = await asyncio.gather(
            hacer_peticion_productos(session),
            hacer_peticion_categorias(session),
            hacer_peticion_perfil(session),
            return_exceptions=True  # Errores como valores, no excepciones
        )
        
        # 2. Procesar resultados
        for resultado in resultados:
            if isinstance(resultado, Exception):
                print(f"‚ùå ERROR: {resultado}")
            else:
                print(f"‚úÖ {resultado['endpoint']}: OK")

# Ejecutar
asyncio.run(cargar_datos())
```

### Componentes clave

1. **`async def`**: Define una coroutine (funci√≥n as√≠ncrona)
2. **`await`**: Pausa la coroutine hasta que la operaci√≥n async termine
3. **`asyncio.gather()`**: Lanza m√∫ltiples coroutines en paralelo
4. **`return_exceptions=True`**: Excepciones se retornan como valores en la lista

### ¬øC√≥mo se maneja el error individual con return_exceptions?

```python
resultados = await asyncio.gather(
    peticion1(),
    peticion2(),  # Esta fallar√° con timeout
    peticion3(),
    return_exceptions=True  # üîë CLAVE
)

# Filtrar exitosos vs errores
exitosos = [r for r in resultados if not isinstance(r, Exception)]
errores = [r for r in resultados if isinstance(r, Exception)]

print(f"‚úÖ Exitosos: {len(exitosos)}")
print(f"‚ùå Errores: {len(errores)}")
```

**Resultado**: Obtienes 2 resultados v√°lidos + 1 excepci√≥n en la lista. No se pierde ning√∫n dato.

### Alternativa: Manejo dentro de cada coroutine

```python
async def peticion_segura(session, url, nombre):
    try:
        async with session.get(url) as response:
            data = await response.json()
            return {"endpoint": nombre, "data": data, "success": True}
    except asyncio.TimeoutError:
        return {"endpoint": nombre, "error": "Timeout", "success": False}
```

**Ventaja**: `gather()` NUNCA lanza excepciones, siempre retorna lista de resultados.

### Ventajas espec√≠ficas para clientes HTTP

| Ventaja | Explicaci√≥n |
|---------|-------------|
| ‚úÖ **C√≥digo m√°s limpio** | Parece c√≥digo s√≠ncrono pero es concurrente |
| ‚úÖ **Sin threads** | M√°s eficiente en I/O intensivo (miles de conexiones simult√°neas) |
| ‚úÖ **Manejo de errores elegante** | `return_exceptions=True` simplifica el manejo |
| ‚úÖ **Escalabilidad** | Puede manejar miles de peticiones concurrentes sin overhead de threads |

### Desventajas espec√≠ficas para clientes HTTP

| Desventaja | Explicaci√≥n |
|-----------|-------------|
| ‚ùå **Requiere librer√≠as async** | `aiohttp` en vez de `requests` (ecosistema distinto) |
| ‚ùå **"Contagio async"** | Toda la cadena de llamadas debe ser `async def` |
| ‚ùå **Curva de aprendizaje** | Conceptos de coroutines y event loop son m√°s complejos |
| ‚ùå **No apto para CPU** | Solo para I/O-bound (peticiones HTTP, DB, archivos) |

---

## Tabla Comparativa: Rendimiento Medido

> **Nota**: Estos son resultados de ejemplo. Ejecuta `benchmark_comparativo.py` para obtener mediciones reales en tu m√°quina.

| Modelo | Promedio | M√≠nimo | M√°ximo | Desviaci√≥n Est√°ndar |
|--------|----------|--------|--------|---------------------|
| Callbacks | 0.125s | 0.118s | 0.135s | 0.006s |
| Futures | 0.123s | 0.116s | 0.132s | 0.005s |
| Async/Await | 0.118s | 0.112s | 0.128s | 0.005s |

### Observaciones

1. **Los 3 modelos tienen rendimiento similar** porque el cuello de botella es la red (I/O-bound)
2. **Async/Await es ligeramente m√°s r√°pido** por evitar el overhead de threads
3. **La diferencia es m√≠nima** para 3 peticiones concurrentes
4. **Async/Await escala mejor** con miles de peticiones

---

## Escenario de Error: Timeout en /categorias

### Pregunta: ¬øQu√© pasa cuando /categorias falla con timeout?

| Modelo | Comportamiento | ¬øSe pierden datos de /productos y /perfil? |
|--------|----------------|-------------------------------------------|
| **Callbacks** | Cada callback maneja su error independientemente | ‚ùå NO - Los dem√°s callbacks se ejecutan normalmente |
| **Futures** | Cada `.result()` con `try/except` individual | ‚ùå NO - Capturas el error y contin√∫as con los dem√°s Futures |
| **Async/Await** | `return_exceptions=True` retorna excepciones como valores | ‚ùå NO - Obtienes lista mixta con resultados + excepciones |

**Conclusi√≥n**: Los 3 modelos permiten obtener resultados parciales sin perder datos por errores individuales, **SI SE MANEJAN CORRECTAMENTE**.

---

## Recomendaci√≥n para EcoMarket

### Contexto del Proyecto

EcoMarket necesita:
- Cargar datos de m√∫ltiples endpoints (productos, categor√≠as, perfil, etc.)
- Mostrar datos en la UI conforme van llegando
- Manejar errores de red sin bloquear la aplicaci√≥n
- Potencial escalabilidad a m√∫ltiples peticiones concurrentes

### Comparaci√≥n de Casos de Uso

| Escenario | Callbacks | Futures | Async/Await |
|-----------|-----------|---------|-------------|
| **3-10 peticiones concurrentes** | ‚úÖ Bueno | ‚úÖ Bueno | ‚úÖ Excelente |
| **100+ peticiones concurrentes** | ‚ö†Ô∏è Overhead threads | ‚ö†Ô∏è Overhead threads | ‚úÖ Ideal |
| **UI reactiva** (mostrar conforme llegan) | ‚úÖ Ideal | ‚úÖ Bueno | ‚úÖ Bueno |
| **Simplicidad de c√≥digo** | ‚ùå Verboso | ‚úÖ Moderado | ‚úÖ Limpio |
| **Ecosistema Python est√°ndar** | ‚úÖ S√≠ (`requests`) | ‚úÖ S√≠ (`requests`) | ‚ö†Ô∏è Requiere `aiohttp` |

### Justificaci√≥n: **Async/Await es la mejor opci√≥n**

#### Razones

1. **C√≥digo m√°s limpio y mantenible**
   - Sintaxis moderna que parece s√≠ncrona pero es concurrente
   - F√°cil de entender para futuros desarrolladores

2. **Escalabilidad**
   - Si EcoMarket crece y necesita manejar cientos de peticiones concurrentes, async/await no requiere refactoring
   - Sin overhead de threads (event loop es m√°s eficiente)

3. **Manejo de errores elegante**
   - `return_exceptions=True` simplifica el c√≥digo de manejo de errores
   - Alternativas con wrappers permiten flujos muy limpios

4. **Ecosistema moderno**
   - `aiohttp` es ampliamente usado y bien mantenido
   - Async/await es el futuro de Python para I/O-bound

#### Cu√°ndo NO usar Async/Await

- Si tu equipo no est√° familiarizado con async y no puede invertir tiempo en aprender
- Si necesitas integrar con librer√≠as que NO son async (fuerza "callback hell" o wrappers complejos)
- Si el proyecto es muy simple (1-2 peticiones) y no se espera crecimiento

#### Cu√°ndo usar Futures en su lugar

- **Proyecto peque√±o con c√≥digo s√≠ncrono existente**: Futures es m√°s f√°cil de integrar
- **Equipo sin experiencia en async**: Futures es m√°s directo
- **Necesitas integrar con librer√≠as bloqueantes**: ThreadPoolExecutor con `requests` funciona bien

#### Cu√°ndo usar Callbacks

- **Eventos altamente reactivos**: Si necesitas responder INMEDIATAMENTE a cada resultado
- **Integraci√≥n con sistemas basados en eventos**: Algunos frameworks usan callbacks nativamente

---

## Conclusi√≥n

Para **EcoMarket**, recomendamos **Async/Await** por su escalabilidad, c√≥digo limpio y rendimiento superior. Sin embargo, si el equipo no est√° listo para async, **Futures** es una excelente alternativa que ofrece un balance entre simplicidad y poder.

