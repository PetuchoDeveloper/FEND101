"""
Benchmark para cliente S√çNCRONO de EcoMarket

Mide el tiempo de ejecuci√≥n de cargar_dashboard_sync() que ejecuta
3 peticiones GET de forma SECUENCIAL (una tras otra).
"""

import time
import sys
import os

# Importar el cliente s√≠ncrono original desde Semana II
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'Semana II', 'ACT9 AI'))

import cliente_ecomarket as client


def cargar_dashboard_sync():
    """
    Versi√≥n S√çNCRONA de cargar_dashboard.
    
    Ejecuta 3 peticiones de forma SECUENCIAL:
    1. listar_productos()
    2. obtener_categorias() - simulado como otra petici√≥n a productos
    3. obtener_perfil() - simulado como obtener el primer producto
    
    Nota: Como la API mock no tiene endpoints /categorias ni /perfil,
    simulamos con operaciones equivalentes.
    """
    resultados = {
        "productos": None,
        "categorias": None,
        "perfil": None
    }
    errores = []
    
    # Petici√≥n 1: Listar productos
    try:
        resultados["productos"] = client.listar_productos()
    except Exception as e:
        errores.append({"endpoint": "productos", "error": str(e)})
    
    # Petici√≥n 2: Categor√≠as (simulado como listar productos con filtro)
    try:
        resultados["categorias"] = client.listar_productos(categoria="frutas")
    except Exception as e:
        errores.append({"endpoint": "categorias", "error": str(e)})
    
    # Petici√≥n 3: Perfil (simulado como obtener producto con ID 1)
    try:
        resultados["perfil"] = client.obtener_producto(1)
    except Exception as e:
        errores.append({"endpoint": "perfil", "error": str(e)})
    
    return {
        "datos": resultados,
        "errores": errores
    }


def ejecutar_benchmark(iteraciones=5):
    """
    Ejecuta el benchmark m√∫ltiples veces y calcula el promedio.
    
    Args:
        iteraciones: N√∫mero de veces a repetir el benchmark
    
    Returns:
        dict: Estad√≠sticas de tiempo
    """
    tiempos = []
    
    print(f"üîÑ Ejecutando benchmark S√çNCRONO ({iteraciones} iteraciones)...")
    print()
    
    for i in range(iteraciones):
        inicio = time.perf_counter()
        
        try:
            resultado = cargar_dashboard_sync()
            fin = time.perf_counter()
            tiempo = fin - inicio
            tiempos.append(tiempo)
            
            num_errores = len(resultado["errores"])
            print(f"  Iteraci√≥n {i+1}: {tiempo:.4f}s (errores: {num_errores})")
        
        except Exception as e:
            print(f"  Iteraci√≥n {i+1}: ERROR - {e}")
    
    if not tiempos:
        return None
    
    promedio = sum(tiempos) / len(tiempos)
    minimo = min(tiempos)
    maximo = max(tiempos)
    
    return {
        "promedio": promedio,
        "minimo": minimo,
        "maximo": maximo,
        "tiempos": tiempos
    }


if __name__ == "__main__":
    print("=" * 60)
    print("BENCHMARK: Cliente S√çNCRONO (requests)")
    print("=" * 60)
    print()
    print("Este benchmark mide el tiempo de cargar 3 endpoints")
    print("de forma SECUENCIAL (uno tras otro).")
    print()
    
    stats = ejecutar_benchmark(iteraciones=5)
    
    if stats:
        print()
        print("üìä RESULTADOS:")
        print(f"  ‚Ä¢ Promedio: {stats['promedio']:.4f}s")
        print(f"  ‚Ä¢ M√≠nimo:   {stats['minimo']:.4f}s")
        print(f"  ‚Ä¢ M√°ximo:   {stats['maximo']:.4f}s")
        print()
        
        # Guardar en archivo para comparaci√≥n
        with open("benchmark_sync_results.txt", "w") as f:
            f.write(f"PROMEDIO={stats['promedio']:.6f}\n")
            f.write(f"MINIMO={stats['minimo']:.6f}\n")
            f.write(f"MAXIMO={stats['maximo']:.6f}\n")
            f.write(f"TIEMPOS={','.join(f'{t:.6f}' for t in stats['tiempos'])}\n")
        
        print("‚úÖ Resultados guardados en benchmark_sync_results.txt")
    else:
        print("‚ùå No se pudo completar el benchmark")
